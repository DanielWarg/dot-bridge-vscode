# Plan: VS Code Extension ".bridge"

## Projektstruktur

Skapa f√∂ljande filstruktur:

```
dot-bridge-vscode/
‚îú‚îÄ‚îÄ .vscode/
‚îÇ   ‚îî‚îÄ‚îÄ launch.json (f√∂r debugging extension)
‚îú‚îÄ‚îÄ .vscodeignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ package.json (VS Code extension manifest)
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ extension.ts (huvudentry point)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollamaService.ts (Ollama API-integration)
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îî‚îÄ‚îÄ diplomat.ts (system prompts)
‚îî‚îÄ‚îÄ agent.md (uppdatera med projektinfo)
```

## Implementation

### 1. package.json
- Extension metadata: name ".bridge", displayName ".bridge - Developer Tone Translator"
- Activation events: `onCommand:bridge.diplomat`, `onCommand:bridge.techspec`
- Commands:
  - `bridge.diplomat` (title: "Bridge: Socialize Text")
  - `bridge.techspec` (title: "Bridge: Generate Tech Spec")
- Keybindings: `cmd+shift+B` / `ctrl+shift+B` ‚Üí `bridge.diplomat`
- Configuration:
  - `contributes.configuration` med title ".bridge"
  - Property `bridge.model`: type "string", default "llama3.2", description "Namnet p√• Ollama-modellen (t.ex. llama3.2, mistral, qwen2.5-coder)"
- Dependencies: `@types/vscode`, `@types/node`, `node-fetch`, `@types/node-fetch`
- Scripts: `compile`, `watch`, `package`

### 2. src/services/ollamaService.ts
- Funktion `bridgeText(userText: string, systemPrompt: string): Promise<string>`
- L√§s modellnamn fr√•n konfiguration: `vscode.workspace.getConfiguration('bridge').get<string>('model', 'llama3.2')`
- POST till `http://localhost:11434/api/generate`
- Request body: `{ model: <fr√•n konfiguration>, prompt: "...", system: "...", stream: false }`
- Error handling: catch fetch errors och returnera "‚ùå Could not connect to local AI. Is Ollama running?"
- Parse response (Ollama returnerar `{ response: "..." }`)

### 3. src/prompts/diplomat.ts
- Exportera konstant `DIPLOMAT_SYSTEM_PROMPT`
- Inneh√•ll: "Du √§r en expert p√• kommunikation f√∂r utvecklare. Skriv om anv√§ndarens text s√• den blir v√§nlig, professionell och tydlig, men beh√•ll den tekniska betydelsen. Var kortfattad."

### 4. src/extension.ts
- `activate()` funktion som registrerar `bridge.diplomat` command
- Command handler:
  1. H√§mta active editor och selected text
  2. Visa fel om ingen text √§r vald via `vscode.window.showErrorMessage()`
  3. Anv√§nd `vscode.window.withProgress()` med:
     - `location: vscode.ProgressLocation.Notification`
     - `title: "Bridging thoughts... üß†"`
     - `cancellable: false`
  4. I progress callback: Anropa `bridgeText()` med text och `DIPLOMAT_SYSTEM_PROMPT`
  5. Ers√§tt markerad text med AI-svar via `editor.edit()`
  6. Visa felmeddelande via `vscode.window.showErrorMessage()` om bridgeText() kastar error

### 5. Konfigurationsfiler
- `tsconfig.json`: VS Code extension TypeScript config
- `.gitignore`: node_modules, out/, *.vsix, .vscode-test/
- `.vscodeignore`: exclude files fr√•n package
- `.vscode/launch.json`: debug configuration f√∂r extension

### 6. Dokumentation
- `README.md`: Installation, usage, requirements (Ollama m√•ste k√∂ra), konfiguration av modell
- Uppdatera `agent.md` med projektinfo enligt mall

## Tekniska Detaljer

- Ollama API: POST till `/api/generate` med `model`, `prompt`, `system`, `stream: false`
- Model selection: L√§ses fr√•n VS Code configuration `bridge.model` (default: "llama3.2") - anv√§ndare kan √§ndra i Settings
- Response parsing: Ollama returnerar `{ response: string }` n√§r `stream: false`
- Error handling: N√§tverksfel ‚Üí tydligt felmeddelande till anv√§ndaren via `vscode.window.showErrorMessage()`
- Progress indication: Anv√§nd `vscode.window.withProgress()` f√∂r professionell laddningsindikator med snurrande ikon

## Testning

- Manuell testning: Starta extension, markera text, k√∂r command
- Verifiera: Text ers√§tts korrekt, felmeddelanden visas n√§r Ollama inte k√∂rs
- Verifiera: Modell kan √§ndras i Settings och anv√§nds korrekt

## Todos

1. Skapa projektstruktur med mappar (src/services, src/prompts) och konfigurationsfiler (package.json, tsconfig.json, .gitignore, .vscodeignore)
2. Implementera ollamaService.ts med bridgeText-funktion som l√§ser modell fr√•n konfiguration och anropar Ollama API med error handling
3. Skapa diplomat.ts med DIPLOMAT_SYSTEM_PROMPT konstant
4. Implementera extension.ts med command registration, vscode.window.withProgress f√∂r progress indication, och text replacement logik
5. Skapa .vscode/launch.json f√∂r debugging extension
6. Skapa README.md med konfigurationsinstruktioner och uppdatera agent.md med projektinformation

